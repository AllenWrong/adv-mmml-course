- date: 1/21
  title: >
    Week 1: <strong>Course introduction</strong>
  slides:
  topics:
    - Course syllabus and requirements
  readings: N/A

- date: 1/28
  title: >
    Week 2: <strong>Cross-modal interactions</strong>
  topics:
    - What are the different ways in which modalities can interact with each other in multimodal tasks? Can we formalize a taxonomy of such cross-modal interactions, which will enable us to compare and contrast them more precisely? <br/>
    - What are the design decisions (aka inductive biases) that can be used when modeling these cross-modal interactions in machine learning models? <br/>
    - What are the advantages and drawbacks of designing models to capture each type of cross-modal interaction? Consider not just prediction performance, but tradeoffs in time/space complexity, interpretability, etc. <br/>
    - Given an arbitrary dataset and prediction task, how can we systematically decide what type of cross-modal interactions exist, and how can that inform our modeling decisions? <br/>
    - Given trained multimodal models, how can we understand or visualize the nature of cross-modal interactions? <br/>
  readings:
    - <a href="https://aclanthology.org/2020.emnlp-main.62/">Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</a> <br/>
    - <a href="https://aclanthology.org/2020.acl-main.469/">What Does BERT with Vision Look At?</a> <br/>
    - <a href="https://openreview.net/pdf?id=rylnK6VtDH">Multiplicative Interactions and Where to Find Them</a> <br/>
    - <a href="https://arxiv.org/abs/2112.12337">Cooperative Learning for Multi-view Analysis</a> <br/>
    - <a href="https://arxiv.org/abs/2109.04448">Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</a> <br/>
    - <a href="https://arxiv.org/abs/2012.12352">Seeing past words&#58; Testing the cross-modal capabilities of pretrained V&L models on counting tasks</a> <br/>
- date: 2/4
  title: >
    Week 3: <strong>Multimodal co-learning</strong>
  topics:
    - What are the types of cross-modal interactions involved to enable such co-learning scenarios where multimodal training ends up generalizing to unimodal testing? <br/>
    - What are some design decisions (inductive bias) that could be made to promote transfer of information from one modality to another? <br/>
    - How do we ensure that during co-learning, only useful information is transferred, and not some undesirable bias? This may become a bigger issue in low-resource settings. <br/>
    - How can we know if co-learning has succeeded? Or failed? What approaches could we develop to visualize and probe the success of co-learning? <br/>
    - How can we formally, empirically, or intuitively measure the additional information provided by auxiliary modality? How can we design controlled experiments to test these hypotheses? <br/>
    - What are the advantages and drawbacks of information transfer during co-learning? Consider not just prediction performance, but also tradeoffs with complexity, interpretability, fairness, etc. <br/>
  readings:
    - <a href="https://arxiv.org/abs/2011.08899">Multimodal Prototypical Networks for Few-shot Learning</a> <br/>
    - <a href="https://arxiv.org/abs/2103.05677">SMIL&#58; Multimodal Learning with Severely Missing Modality</a> <br/>
    - <a href="https://arxiv.org/abs/2107.13782">Multimodal Co-learning&#58; Challenges, Applications with Datasets, Recent Advances and Future Directions</a> <br/>
    - <a href="https://arxiv.org/abs/2010.06775">Vokenization&#58; Improving Language Understanding with Contextualized, Visual-Grounded Supervision</a> <br/>
    - <a href="https://arxiv.org/abs/2106.04538">What Makes Multi-modal Learning Better than Single (Provably)</a> <br/>
    - <a href="https://arxiv.org/abs/1812.07809">Found in Translation&#58; Learning Robust Joint Representations by Cyclic Translations Between Modalities</a> <br/>
    - <a href="https://arxiv.org/abs/1301.3666">Zero-Shot Learning Through Cross-Modal Transfer</a> <br/>
    - <a href="https://arxiv.org/abs/1912.02315">12-in-1&#58; Multi-Task Vision and Language Representation Learning</a> <br/>
    - <a href="https://arxiv.org/abs/1906.03926">A Survey of Reinforcement Learning Informed by Natural Language</a> <br/>
  
- date: 2/11
  title: >
    Week 4: <strong>Pretraining paradigm</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 2/18
  title: >
    Week 5: <strong>Multimodal reasoning</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 2/25
  title: >
    Week 6: <strong>Memory and long-term interactions</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 3/4
  title: >
    Week 7: <strong>No classes – Spring break</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 3/11
  title: >
    Week 8: <strong>No classes – Spring break</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 3/18
  title: >
    Week 9: <strong>Brain and multimodal perception</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 3/25
  title: >
    Week 10: <strong>Beyond language and vision</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 4/1
  title: >
    Week 11: <strong>Subjectivity and dataset biases</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 4/8
  title: >
    Week 12: <strong>No classes – CMU Carnival</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 4/15
  title: >
    Week 13: <strong>Fairness and real-world constraints</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 4/22
  title: >
    Week 14: <strong>Multimodal generalization</strong>
  slides:
  topics:
    - TBD
  readings: TBD
  
- date: 4/29
  title: >
    Week 15: <strong>Low-resource settings</strong>
  slides:
  topics:
    - TBD
  readings: TBD
