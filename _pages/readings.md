---
layout: page
permalink: /spring2022/readings/
title: Readings
---

Week 2: Cross-modal interactions

Discussion probes:
- What are the different ways in which modalities can interact with each other in multimodal tasks? Can we formalize a taxonomy of such cross-modal interactions, which will enable us to compare and contrast them more precisely?
- What are the design decisions (aka inductive biases) that can be used when modeling these cross-modal interactions in machine learning models?
- What are the advantages and drawbacks of designing models to capture each type of cross-modal interaction? Consider not just prediction performance, but tradeoffs in time/space complexity, interpretability, and so on.
- Given an arbitrary dataset and prediction task, how can we systematically decide what type of cross-modal interactions exist, and how can that inform our modeling decisions?
- Given trained multimodal models, how can we understand or visualize the nature of cross-modal interactions?

Relevant papers:
- Additive interactions: [Does my multimodal model learn cross-modal interactions? Itâ€™s harder to tell than you might think!](https://aclanthology.org/2020.emnlp-main.62/)
- Grounding interactions: [What Does BERT with Vision Look At?](https://aclanthology.org/2020.acl-main.469.pdf)
- Multiplicative interactions: [Multiplicative Interactions and Where to Find Them](https://openreview.net/pdf?id=rylnK6VtDH)
- Cooperative interactions: [Cooperative Learning for Multi-view Analysis](https://arxiv.org/pdf/2112.12337.pdf)
- Visualizations and ablations: [Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](https://arxiv.org/abs/2109.04448)
- Visualizations and ablations: [Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks](https://arxiv.org/abs/2012.12352)
